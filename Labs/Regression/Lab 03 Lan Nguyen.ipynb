{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e0c1e1",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "\n",
    "#1. Based on the value of MSE alone, then the decesion tree model should be the better choice. This could be because the tree take every predictors into account and make layers of decision, instead of taking one or two predictor and creating a model. But the performance of the models is not based on MSE alone.\n",
    "\n",
    "#2. If we increase the number of k into a large number, the model seems to also does worse on the fitting, but if we leave a small number like 3 or 4, the MSE would gave us also a large value. So, the best value seems to be between 4 and a large number, and I found 7 and 8 to have the smallest value of error. \n",
    "\n",
    "#3. Based on importancy using external codes, I deduct MedInc to be the most important feature to predict. Usually, the decision tree model have the most important feature as the first node, and goes from there based on importance, so people can predict which predictor is the more important predictor for the model based on hierachy. \n",
    "\n",
    "#4. K-nn should be the fastest model to train, because it stores the dataset. This also shows K-nn superior when it comes to required resources, since it uses the store data to predict new instance, instead of putting the instance into the model.\n",
    "\n",
    "#5. New unseen data would be better on decision tree model, since if we have a data that is outside the dataset, a decision tree would better predict the new data. Linear regression when it comes to new data, it would have overfitting if the data goes beyond the bound of the training dataset. \n",
    "\n",
    "### Step 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "419a72f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature1  Feature2  Feature3     Target\n",
      "0   3.745401  3.059264  0.244076  10.830159\n",
      "1   9.507143  0.697469  0.990354   9.070235\n",
      "2   7.319939  1.460723  0.068777  17.431063\n",
      "3   5.986585  1.831809  1.818641  10.351300\n",
      "4   1.560186  2.280350  0.517560   9.214018\n",
      "5   1.559945  3.925880  1.325045  13.140441\n",
      "6   0.580836  0.998369  0.623422   7.113863\n",
      "7   8.661761  2.571172  1.040136  17.032955\n",
      "8   6.011150  2.962073  1.093421   6.118260\n",
      "9   7.080726  0.232252  0.369709  19.803304\n",
      "10  0.205845  3.037724  1.939169  16.583672\n",
      "11  9.699099  0.852621  1.550266   7.980735\n",
      "12  8.324426  0.325258  1.878998   5.082832\n",
      "13  2.123391  4.744428  1.789655  17.231921\n",
      "14  1.818250  4.828160  1.195800  15.602860\n",
      "15  1.834045  4.041987  1.843748  15.935108\n",
      "16  3.042422  1.523069  0.176985  16.569055\n",
      "17  5.247564  0.488361  0.391966   6.110670\n",
      "18  4.319450  3.421165  0.090455  10.376986\n",
      "19  2.912291  2.200762  0.650661   6.738036\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# Create a synthetic dataset with 20 rows\n",
    "data = pd.DataFrame({\n",
    " 'Feature1': np.random.rand(20) * 10,\n",
    " 'Feature2': np.random.rand(20) * 5,\n",
    " 'Feature3': np.random.rand(20) * 2,\n",
    " 'Target': np.random.rand(20) * 15 + 5 # Random target variable\n",
    "})\n",
    "# Display the dataset\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179c869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = data[['Feature1', 'Feature2', 'Feature3']]\n",
    "y = data['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc4aef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 14.033670568021368\n"
     ]
    }
   ],
   "source": [
    "# Create Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ffd672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 11.412907196317398\n"
     ]
    }
   ],
   "source": [
    "# Create k-Nearest Neighbors Regression model\n",
    "k_neighbors = 8  # You can adjust the number of neighbors as needed\n",
    "model_1 = KNeighborsRegressor(n_neighbors=k_neighbors)\n",
    "model_1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_1.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78ab6cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 47.64058414749795\n"
     ]
    }
   ],
   "source": [
    "# Create Decision Tree model\n",
    "model_2 = DecisionTreeRegressor(random_state=42)\n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model_2.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b347f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothetical Values: [{'Feature1': 8.0, 'Feature2': 3.5, 'Feature3': 1.0}]\n",
      "Linear Regression Prediction: 14.37\n",
      "k-Nearest Neighbors Prediction: 11.77\n",
      "Decision Tree Prediction: 15.60\n"
     ]
    }
   ],
   "source": [
    "# Hypothetical values for features\n",
    "hypothetical_data = pd.DataFrame({\n",
    " 'Feature1': [8.0],\n",
    " 'Feature2': [3.5],\n",
    " 'Feature3': [1.0]\n",
    "})\n",
    "# Predictions using the trained models\n",
    "linear_prediction = model.predict(hypothetical_data)\n",
    "knn_prediction = model_1.predict(hypothetical_data)\n",
    "dt_prediction = model_2.predict(hypothetical_data)\n",
    "# Display the predictions\n",
    "print(f\"Hypothetical Values: {hypothetical_data.to_dict(orient='records')}\")\n",
    "print(f\"Linear Regression Prediction: {linear_prediction[0]:.2f}\")\n",
    "print(f\"k-Nearest Neighbors Prediction: {knn_prediction[0]:.2f}\")\n",
    "print(f\"Decision Tree Prediction: {dt_prediction[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee3b5a",
   "metadata": {},
   "source": [
    "#8.\n",
    "The k-nn model seems to work the best for this dataset. Linear Regression came in second, while Decision Tree Prediction showed the worst prediction. I think K-nn works best because the error when the model was first created seems to be the lowest, as well as the prediction I gave was 8 nearest neighbor, I think the number has the lowest MSE when it comes to number of neighbors between 1 and 9. K-nn is also easier to recognize the concept when it comes to modelling. \n",
    "\n",
    "#9\n",
    "When using the knn model, the prediction gave was 11.77, while Linear Regression Prediction model gave 14.37, and the Decision Tree predicted 15.60, which is the largest value.\n",
    "\n",
    "Linear regression is the simpliest model out of the three, but its weaknesses would be when the instance gave has the value over or under the value range exist in the dataset.\n",
    "\n",
    "K-nn was the best when it comes to accuracy, since it based its prediction on exist data points that are near the new instance. Its weakness would be the model needs to a certain number of neighbors to predict, and if the number is too big or too small, the prediction changed drastically. \n",
    "\n",
    "Decision tree would be best if the data is outside the existance dataset range, but its weakness would be the data can easily be overfit or underfit, because the prediction value is based on the average of the prediction node. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
